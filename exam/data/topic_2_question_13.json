{
  "question_text": "You have a Fabric workspace that contains a lakehouse named Lakehouse1.<br/>In an external data source, you have data files that are 500 GB each. A new file is added every day.<br/>You need to ingest the data into Lakehouse1 without applying any transformations. The solution must meet the following requirements<br/>Trigger the process when a new file is added.<br/>Provide the highest throughput.<br/>Which type of item should you use to ingest the data?",
  "question_images": [],
  "answers": {
    "A": "Eventstream",
    "B": "Dataflow Gen2",
    "C": "Streaming dataset",
    "D": "Data pipeline"
  },
  "answer_images": {},
  "correct_answer": [
    "D"
  ],
  "correct_answer_images": [],
  "discussion": "Eventstream is designed for ingesting real-time or streaming data from sources like IoT devices or logs. It’s not optimized for batch processing or large files.\n\nD. Data pipeline.\n\nData pipelines are designed to handle large volumes of data efficiently and can be configured to trigger the ingestion process automatically when new files are added to the external data source. They also provide high throughput, making them suitable for handling 500 GB files daily without applying any transformations.\n\nNotice that the same question is below (#14) and also as #16 topic 3. Comparing the answers, I would choose Dataflow Gen 2, the only answer that appears in each set with answers.\n\nI just took the exam. The options to choose from are different now:\nEventstream.\nDataflow\nActivator\nNotebook\n\nWich one do you guess is correct?\n\nActivator is the tool that alerts when a file has been added. It would then trigger a Data Pipeline to actually move the data. \nDepending on the new wording of the question, this it likely the correct answer.  Notebook is viable but overkill in effort.\n\nNotebook\n\nd is better for 500 GB files\n\nI would prefer using data pipeline although it is a preview feature. Eventstream and streaming data are designed for realtime events\n\nFor high-throughput, event-triggered ingestion of large files into a lakehouse without transformations, Data pipeline is the most appropriate and efficient item in Fabric.\n\nStreaming dataset is the only answer that ticks the requirements for storage trigger and high throughput.\nData Pipeline is not right as at 2025-Jan. The storage trigger is still in preview, so it doesn't satisfy the requirement. But it's probably the best option.",
  "question_type": "multiple_choice",
  "exam": "dp-700",
  "topic": 2,
  "question_number": 13,
  "url": "https://www.examtopics.com/discussions/microsoft/view/152972-exam-dp-700-topic-2-question-13-discussion/"
}