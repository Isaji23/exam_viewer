{
  "question_text": "You have a Fabric workspace that contains a lakehouse named Lakehouse1.<br/><br/>In an external data source, you have data files that are 500 GB each. A new file is added every day.<br/><br/>You need to ingest the data into Lakehouse1 without applying any transformations. The solution must meet the following requirements:<br/><br/>‚Ä¢\tTrigger the process when a new file is added.<br/>‚Ä¢\tProvide the highest throughput.<br/><br/>Which type of item should you use to ingest the data?",
  "question_images": [],
  "answers": {
    "A": "KQL queryset",
    "B": "Streaming dataset",
    "C": "Notebook",
    "D": "Dataflow Gen2"
  },
  "answer_images": {},
  "correct_answer": [
    "D"
  ],
  "correct_answer_images": [],
  "discussion": "The best choice is D. Dataflow Gen2.\nWhy Dataflow Gen2?\n- Optimized for large-scale ingestion: Handles high-throughput data ingestion efficiently.\n- Supports automatic triggers: Can detect new files and start ingestion immediately.\n- No transformations required: Dataflow Gen2 allows direct ingestion into Lakehouse1 without modification.\nWhy Not Other Options?\n- A. KQL queryset ‚Üí Used for querying data, not for ingestion.\n- B. Streaming dataset ‚Üí Designed for real-time reporting, not batch ingestion.\n- C. Notebook ‚Üí Requires manual execution or scheduling, making it less efficient for automated ingestion.\nFor more details, check out Microsoft's documentation on Dataflow Gen2 ingestion in Fabric. üöÄ Let me know if you need further clarification!\n\nAnswer: C. Notebook\nWhy Notebook?\nFor ingesting very large files (e.g., 500 GB per file, daily) into a Lakehouse with no transformations, and to trigger when a new file arrives while achieving the highest throughput, the best-fit item is a Notebook (or Spark Job Definition) using Spark Structured Streaming.\n\nBecause the requirement explicitly says ‚ÄúProvide the highest throughput‚Äù for 500 GB daily files, the Spark-based notebook is the most defensible choice.\n\nmy apologies \nCorrect = \"D\"\n\nWhile the files might need to be specified explicitly, this question seems to be hinting heavily to knowing about FastCopy as part of Dataflow Gen2. (For files > 5MB, limited transformations).\n\nIn a real scenario I would use a Data Pipeline. Notebook seems overkill unless the file type is not supported by FastCopy.\n\nI am team C. The throughput requirement is better met by a notebook, and for on demand ingestion when a file arrives, I need to embed the solution into a pipeline anyway. Remember: The file arrives at an external file storage. There is no native Fabric event for this. We need an external trigger.\n\nB cannot be data flow, it does not require transformation and the notebook is manual or semi-automatic, the streaming data set is designed for continuous and rapid ingestion and is automatically activated when a new file arrives.\n\nDataflow Gen 2\n\nThe Dataflow Gen2 is the best, and basically because of \"Trigger automatically when a new file arrives\". A DFGEN2 supports event-based triggers, where a notebook needs to be added to a pipeline, this pipeline needs to be configured with an event-based trigger, ... \n\nSo, notebook DOES provide high throughput, but requires too much extra configuration. If you're looking at the stand-alone items, it's 100% Dataflow Gen2\n\nNO DFGEN2 DOESN'T supports event-based triggers\n\npor el tama√±o del archivo\n\nOffers full control, supports large-scale data ingestion, integrates well with triggers, and maximizes throughput",
  "question_type": "multiple_choice",
  "source": "direct_html",
  "url": "https://www.examtopics.com/discussions/microsoft/view/304566-exam-dp-700-topic-3-question-16-discussion/",
  "topic": 3,
  "question_number": 16
}