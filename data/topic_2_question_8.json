{
  "question_text": "HOTSPOT -<br/>You have an Azure Event Hubs data source that contains weather data.<br/>You ingest the data from the data source by using an eventstream named Eventstream1. Eventstream1 uses a lakehouse as the destination.<br/>You need to batch ingest only rows from the data source where the City attribute has a value of Kansas. The filter must be added before the destination. The solution must minimize development effort.<br/>What should you use for the data processor and filtering? To answer, select the appropriate options in the answer area.<br/><br/>NOTE: Each correct selection is worth one point.<br/><img src=\"https://img.examtopics.com/dp-700/image37.png\" title=\"image41\"/>",
  "question_images": [
    "https://img.examtopics.com/dp-700/image37.png"
  ],
  "answers": {},
  "answer_images": {},
  "correct_answer": [],
  "correct_answer_images": [
    "https://img.examtopics.com/dp-700/image38.png"
  ],
  "discussion": "1. eventstream with an external data source\n2. eventstream processor\nhttps://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/add-source-azure-event-hubs?pivots=enhanced-capabilities\nhttps://learn.microsoft.com/en-us/fabric/real-time-intelligence/event-streams/process-events-using-event-processor-editor?pivots=enhanced-capabilities\n\nThe answer is correct. The question states that eventstream already exists and uses the lakehouse as destination. The question also states that the rows need to be batch ingested. Thus 1) Dataflow and 2) Filter activity are the best in this situation\n\nTo minimize development - eventstream, eventstream processor - filter\n\nA very sneaky question but the use of data flows are the correct answer. The event stream is ingesting ALL weather data live and storing with the Lakehouse. We can then assume that the source being used by the event stream matches the data in the lakehouse. It then asks about filtering it to only include Kansas. Given the data is already in the lakehouse the answers provide make sense\n\nIf the requirement is for scheduled batch ingestion (e.g., hourly imports), then the eventstream approach isn't ideal—you’d typically use a Dataflow Gen2 or data pipeline for scheduled, batched ETL.\n\nbut real-time stream ingestion with inline filtering (i.e., data continuously flows but only rows matching City = \"Kansas\" pass through), then using an eventstream with an event processor is valid and requires minimal development effort.\nI think correct answer is \n1. eventstream with an external data source\n2. eventstream processor\nand the phrase “batch ingest only rows” in the question is the tricky part.\n\nI think sevond one has to be eventstream processor, but why is the first one eventstream with an external datasource? Lakehouse is a default data source in eventstream\n\nSince the question specifies \"batch ingest only rows\", that changes the approach. Real-time eventstream processors are typically used for streaming ingestion, not batch.                                                 Data Processor\tA Dataflow Gen2 dataflow\tDataflow Gen2 supports batch processing and can connect to Event Hubs via staging or intermediate storage.\nFiltering\tA filter in a Dataflow Gen2 dataflow\tYou can apply a row-level filter (e.g., City == \"Kansas\") in the transformation steps of the dataflow.\n\n1. eventstream with an external data source\n2. eventstream processor\n\nCorrect Solution\nData processor: An eventstream with an external data source\nFiltering: An eventstream processor\n\nData processor: An eventstream with a custom endpoint\nEventstreams allow real-time processing of data using processors, and a custom endpoint provides flexibility in routing data downstream, especially after transformations or filters.\n\nFiltering: An eventstream processor\nEventstream processors are used within the eventstream pipeline to apply transformations, filters (like your \"City = 'Kansas'\" logic), or aggregations before writing to a destination. This is the most efficient and low-code way to apply this filter.\n\nData processor: An eventstream with a custom endpoint\nEventstreams allow real-time processing of data using processors, and a custom endpoint provides flexibility in routing data downstream, especially after transformations or filters.\n\nFiltering: An eventstream processor\nEventstream processors are used within the eventstream pipeline to apply transformations, filters (like your \"City = 'Kansas'\" logic), or aggregations before writing to a destination. This is the most efficient and low-code way to apply this filter.\n\n1. eventstream with an external data source\n2. eventstream processor    . Dataflow  not support  Event Hubs\n\nThe provided answer. This question is asking how you can batch ingest only rows from the data source where the City attribute has a value of Kansas. To minimize development effeort, the data processor must be DataFlow Gen2 and the filtering should use the Filter in DataFlow Gen2\n\nthe problem is \"You need to batch ingest only rows from the data source where the City attribute has a value of Kansas.\" where is the data source in this one? it is very hard to build dataflow Gen2 if this data source is event hub. key word \"batch ingest\" looks like point to dataflow Gen2. And question said they already set eventstream to save data in lakehouse. If this data source means the table in lakehouse, then I can agree dataflow Gen2 is best option.",
  "question_type": "image_only",
  "source": "direct_html",
  "url": "https://www.examtopics.com/discussions/microsoft/view/155368-exam-dp-700-topic-2-question-8-discussion/",
  "topic": 2,
  "question_number": 8
}