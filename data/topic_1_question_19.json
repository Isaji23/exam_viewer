{
  "question_text": "HOTSPOT -<br/>You have a Fabric workspace named Workspace1 that contains the items shown in the following table.<br/><img src=\"https://img.examtopics.com/dp-700/image10.png\" title=\"image11\"/><br/>For Model1, the Keep your Direct Lake data up to date option is disabled.<br/>You need to configure the execution of the items to meet the following requirements:<br/>Notebook1 must execute every weekday at 8:00 AM.<br/>Notebook2 must execute when a file is saved to an Azure Blob Storage container.<br/>Model1 must refresh when Notebook1 has executed successfully.<br/>How should you orchestrate each item? To answer, select the appropriate options in the answer area.<br/>NOTE: Each correct selection is worth one point.<br/><img src=\"https://img.examtopics.com/dp-700/image11.png\" title=\"image12\"/>",
  "question_images": [
    "https://img.examtopics.com/dp-700/image10.png",
    "https://img.examtopics.com/dp-700/image11.png"
  ],
  "answers": {},
  "answer_images": {},
  "correct_answer": [],
  "correct_answer_images": [
    "https://img.examtopics.com/dp-700/image12.png"
  ],
  "discussion": "All four answers are correct.\n\nNotebook1: From Real-Time hub, configure the execution of Notebook1.\nNotebook2: Add Notebook2 to Pipeline1.\nPipeline1: Configure the execution of Pipeline1 by using a schedule.\nModel1: Add Model1 to Pipeline1.\n\nNotebook 1 runs at 8AM, this can be scheduled via Pipeline (Pipeline1), so the configuration of scheduling is in Pipeline 1. Because Model1 refreshes on completion of Notebook1, it should be added to Pipeline 1 after Notebook 1. Notebook 2 is separate to the Pipeline 1 sequence, it requires a trigger (file saved in Blob) from real-time hub.\n\nTotally agree with the answer.\nN1 - Pipeline\nN2 - Real-time Hub\nPipeline - Schedule\nModel - Add to pipeline\n\nNotebook1\nMust run every weekday at 8:00 AM → that is a scheduled job.\nBest option: Add Notebook1 to an Apache Spark job definition (because Spark jobs can be scheduled).\nNotebook2\nMust run when a file arrives in Azure Blob Storage.\nThat’s an event-based trigger from Real-Time hub.\nBest option: From Real-Time hub, configure the execution of Notebook2.\nPipeline1\nNo special condition given, but it is listed in the items table.\nSince it’s a pipeline, its execution must be tied to something. The natural match is scheduled pipelines (not Spark job).\nBest option: Configure the execution of Pipeline1 by using a schedule.\nModel1\nMust refresh when Notebook1 finishes successfully.\nThat means the refresh should be tied to Notebook1’s job completion.\nBest option: Add Model1 to Pipeline1 (since pipelines can orchestrate dependent tasks in order).\n\nNotebook1 - 2) Add Notebook1 to Pipeline1 (Allows you to schedule execution at 8:00 AM every weekday. Pipelines offer precise scheduling.)\nNotebook2 - 3) From Real-Time hub, configure the execution of Notebook2 (Enables event-driven triggers, such as reacting to file uploads in Azure Blob Storage.)\nPipeline1 - 2) Configure the execution of pipeline1 by using a schedule (Pipelines can be scheduled directly without embedding them in Spark jobs.)\nModel1 - 1) Add Model1 to Pipeline1 (Since Direct Lake refresh is disabled, pipeline-based execution triggered post-Notebook1 ensures Model1 refreshes on success.)\n\nNB1 add to Pipeline\nNB2 realtime hub\nPipeline: Schedule\nM1: add to Pipeline\n:\nIn Microsoft Fabric, the Real-Time hub allows you to ingest and manage streaming data from various sources, including Azure Blob Storage events. You can create event streams in Fabric that connect to your Azure Blob Storage account and trigger actions based on events like file creation, deletion, or modification. These event streams can then be used to process and analyze the real-time data for various applications like data analysis, machine learning, or creating alerts\n\nBased on the information provided, I believe A is correct.",
  "question_type": "image_only",
  "source": "direct_html",
  "url": "https://www.examtopics.com/discussions/microsoft/view/153607-exam-dp-700-topic-1-question-19-discussion/",
  "topic": 1,
  "question_number": 19
}